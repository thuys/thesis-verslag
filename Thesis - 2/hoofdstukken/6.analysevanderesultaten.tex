\chapter{Analyse van de resultaten}
In dit hoofdstuk zullen de resultaten van het vorige hoofdstuk aanbod komen en de mogelijke oorzaken besproken worden. 

Daarnaast worden de verschillende systemen met elkaar vergeleken wat bepaalde keuzes . 
\section{Calibratie}
Over de calibratie testen valt in het algemeen niet veel af te leiden, deze testen zijn niet uitgevoerd op een volledig dezelfde infrastructuur, zo heeft Pgpool-II slechts 3 instanties t.o.v. 6 voor MongoDB, maar dat was ook niet het doel. 

Enkel op de stijgende variatie van de vertagingen in MongoDB zal dieper ingegaan worden. De reden hiervoor is een lezers/schrijving locking systeem op een gehele database\cite{mongodb-concurrency}. Hierdoor zorgt een leesactie voor de blokkering van alle schrijfactie op de database en vice versa. Naar mate er meer gebruikers zijn, kunnen er meer opeenvolgende schrijfoperaties zijn, dit zal de leesacties langer blokkeren. Maar indien alle gebruikers samen lezen, kan dit parallel gebeuren, een grotere variatie in de vertraging treedt hierdoor op. 

De netwerkinfrastructuur heeft een gemiddelde ping van 0.4ms naar elke node ($\sigma = 0.2$ bij 10 000 ping's). 

\section{Beschikbaarheidstest}
Bij de beschikbaarheidstesten blijkt er uit de resultaten dat de verschillende systemen een andere aanpak hebben genomen. Deze zullen nu verder in detail besproken worden. Belangrijk  is dat er hier bewerking aan de basisbelasting uitgevoerd worden, waardoor er data van de verschillende datadistributies gelezen zal worden. 

\paragraph{HBase} Bij HBase heeft een bepaalde RegionServer de verantwoordelijkheid over een Regio voor een bepaalde tijd. Dit is een sessie dit door HMaster uitgedeeld wordt en bijgehouden wordt in Zookeeper. Deze sessie kan vroegtijdig beëindigd worden of er moet gewacht worden tot deze verlopen is, enkel op die momenten kan er een nieuwe RegionServer aangeduid worden. Dit zorgt voor een duidelijk verschil tussen een zachte stop, een harde stop of netwerk probleem. 

De duur van een sessie kan geconfigureerd worden in Zookeeper en staat standaard op 180 seconden. \cite{hbase-doc}. 

\subparagraph{HBase: Zachte stop} Bij een zachte stop, is er slechts af en toe sprake dat dit merkbaar is, de verklaring hiervoor is dat dit enkel wordt opgemerkt als de RegionServer die op dat moment verantwoordelijke is voor de Region wordt stopgezet. In de testen is dit niet zichtbaar omdat er verschillende opeenvolgende queries worden uitgevoerd. \\
Indien deze RegionServer wordt stopgezet, nemen de queries tijdelijk meer tijd in beslag.  Het terug online brengen van de server heeft geen invloed op de snelheid een query wordt uitgevoerd . Na het stopzetten van de RegionServer is er in bepaalde gevallen een verhoogde vertraging in beide leesoperaties (scan en lees). \\
Zodra er een herverdeling is van de Regions over de aanwezige Regionservers, verdwijnt deze verhoogde vertraging. 

\subparagraph{Netwerk onderbreking} Bij een netwerk onderbreking, worden de queries tijdelijk stopgezet en falen de queries in tussentijd. Deze onderbreking duurt significant langer dan in het geval van een zachte stop. Dit komt doordat de regio's pas kunnen toegewezen worden na het verlopen van hun sessie.  

\subparagraph{HBase: Harde stop} Bij het stopzetten van een instantie op de harde manier, zijn er 2 gedragen: het eerste is gelijk aan deze van een netwerk onderbreking. De andere laat pas opnieuw queries toe na het herstellen van het netwerk verkeer. In een manuele test bleek dit opgelost te zijn na het verbreken van de connectie en het opnieuw verbinden, maar de oorzaak waarom dit slecht af en toe voorkomt, is niet gevonden. 

\subparagraph{Herstel van de instantie} Het herstel van de server zal automatisch op een asynchrone manier gebeuren. Er valt ook te configureren hoeveel data er maximaal per seconde zal worden gesynchroniseerd. Het herstel is niet merkbaar voor de gebruiker bij de testen. 

\paragraph{MongoDB} Bij MongoDB is er tussen de leden van een Replicaset een heartbeat protocol. Indien er gedurende 10 seconden geen antwoord op een een heartbeat komt, wordt een server als offline bestempeld \cite{mongodb-manual}. Dit heeft opnieuw zijn invloed op de verschillende manieren om een server stop te stopzetten. 

\subparagraph{Zachte stop en harde stop} Bij een zachte of harde stop is er een kans van 1 op 3 dat het uitvallen van een instantie zichtbaar is, dit is te verklaren doordat enkel het uitschakelen van de primary een invloed zal hebben op de vertraging, in de standaard modus werd er enkel gelezen naar en geschreven van de primary. Nadien is er geen invloed bij de verschillende queries naar de vertraging. De reden dat een harde stop op dezelfde manier reageert kan verklaard worden op 2 wijzen: bij een harde stop wordt de sessie nog altijd vrijgegeven, of er zijn verkiezingen voor een nieuwe primary voordat de sessie van de oude primary is afgelopen. 

\subparagraph{Netwerk onderbreking} Bij een netwerk onderbreking zou het te verwachten zijn dat na 10 seconde een primary zou veranderen. Onder de aangelegde belasting blijkt dat de database heel de tijd onbeschikbaar tot het opnieuw beschikbaar maken van het netwerk. Bij het manueel testen blijkt dat er dezelfde foutmelding gegeven wordt als bij het stoppen van de database, maar dat de data onbeschikbaar is. Het volstaat om de verbindingen af te sluiten en opnieuw aan te maken om het probleem op te lossen.

\subparagraph{Herstel van de instantie} Het herstel van de server zal automatisch op een asynchrone manier gebeuren. Dit is niet merkbaar voor de gebruikers. 

\paragraph{Pgpool-II} Bij Pgpool-II wordt er bij het hebben van een connectie naar Pgpool-II, de connecties naar de verschillende PostgreSQL instanties gecontroleerd. Bij het uitvallen van een instantie en opnieuw opstarten terwijl er geen gebruiker verbonden is met Pgpool-II, zal dit niet opgemerkt worden. Daarnaast zijn er wel verschillende reacties op de geteste scenario's. 

Een vereiste bij het herstellen van een instantie is dat er op dat moment geen enkele gebruiker actief is. 

\subparagraph{Zachte stop} Bij een zachte stop van een data instantie worden alle verbinden met Pgpool-II verbroken, nadien kan er terug verbonden worden met Pgpool-II. In deze omgeving gaan nadien de verschillende schrijfoperaties sneller omdat deze niet meer gerepliceerd moeten worden, bij een grote hoeveelheid data instanties zal dit effect kleiner worden.  

\subparagraph{Harde stop} Een harde stop reageert hetzelfde als een zachte stop, dit omdat ook hier de connecties onmiddellijk verbroken zijn, de data instantie zal antwoorden dat er geen service op de poort aan het luisteren is. 

\subparagraph{Netwerk onderbreking} Bij een netwerk onderbreking is er een ander gedrag, de queries wachten op een antwoord maar krijgen dit niet. Hierdoor wordt er gewacht op de time-out die standaard 30 seconde is. Na deze tijd worden connecties verbroken, kan er opnieuw verbonden worden en kunnen queries opnieuw uitgevoerd worden. 

\subparagraph{Vermindering van leesvertraging} De reden tot de vermindering van leesvertraging is te vinden in de manier dat Pgpool-II de replicatie van de queries doet. Deze zullen eerst op de master uitgevoerd worden en vervolgens op de slaves. Bij het wegvallen van een instantie is er nog maar een enkele data server over, hierdoor duurt een schrijfactie maar half zo lang. De leesacties duren ongeveer even lang aangezien dezelfde acties nog steeds genomen worden. 

\subparagraph{Herstel van de instantie} Bij het opnieuw inschakelen van een instantie dient in Pgpool-II het herstel handmatig in gang gezet te worden. De data zal van de master naar de instantie gesynchroniseerd worden. In het geval van een grote achterstand zal dit merkbaar zijn omdat het proces aan maximale snelheid wordt uitgevoerd; een grote belasting op de CPU, harde schijf en het netwerk kunnen dus voorkomen. Om het herstel te voltooien moeten alle connecties naar Pgpool-II op een gegeven moment gesloten worden. In de testen die werden uitgevoerd waren er steeds actief en hierdoor slaagde het herstel niet. 

\paragraph{Conclusie} Hoewel er verschillende reacties zijn tussen HBase en MongoDB, ligt de interne werking vrij dicht bij elkaar, de status van beiden wordt permanent opgevolgd. Bij MongoDB gebeurt dit door de data instanties zelf en kan de parameter niet aangepast worden. Bij HBase is er een extern systeem voor gebruikt waarbij de parameter geconfigureerd worden. Pgpool-II heeft een heel ander systeem door enkel de instanties te controleren op het moment dat er een verbinding is. 
Daarnaast ondersteunt Pgpool-II ook niet de automatische herstel en komt de handmatige herstel niet tot voltooiing onder constant gebruik, hiervoor zijn beide andere systemen automatischer. Een overzicht van het gedrag bij het stoppen en starten van een instantie , bevinden zich in tabel \ref{table:beschikbaarheid-stop-resultaat} en \ref{table:beschikbaarheid-herstel-resultaat}.

\begin{table}[htbp]
  \centering
    \begin{tabular}{l | lll}
          & Zachte stop & Harde stop & Netwerk stop \\
    \hline
    \multirow{2}{*}{HBase} & Enkele seconden & Tiental seconden & Tiental seconden \\
    & & of onbeperkt&  \\
    \multirow{2}{*}{MongoDB} & 1/3 van de gevallen, & 1/3 van de gevallen, & Enkele seconden tot \\
    & enkele seconden & enkele seconden & Onbeperkt\\
    Pgpool-II & Enkele seconden & Enkele seconden & 30 seconden \\
    \end{tabular}%
    \caption{Beschikbaarheid: Overzicht van de reacties bij het stoppen van een instantie }
  \label{table:beschikbaarheid-stop-resultaat}%
\end{table}

\begin{table}[htbp]
  \centering
    \begin{tabular}{l|l}
          & Automatisch herstel \\
    \hline
    HBase & Ja \\
    MongoDB & Ja \\
    Pgpool-II & Nee \\
    \end{tabular}%
      \caption{Beschikbaarheid: Overzicht van de ondersteuning van automatisch herstel}
  \label{table:beschikbaarheid-herstel-resultaat}%
\end{table}%



\section{Consistentie test}
\paragraph{HBase} HBase garandeert strikte consistentie op een enkel record en hoe deze garantie tot uitvoering wordt gebracht, is duidelijk zichtbaar in figuur \ref{fig:consistentie-hbase-R1}. Een lees query wordt namelijk op wacht gezet tot de schrijf query voltooid is, dit valt af te lezen doordat de lijn van het stoppen met schrijven een hogere waarde heeft als het stoppen met lezen en dit zo de hele tijd is, daarnaast volgt het stoptijdstip van de leesbewerking deze van de schrijfwerking. Tenslotte kan ook de data in meer detail bekeken worden en is dit zichtbaar. In figuur \ref{fig:consistency-hbase-uitleg} wordt het lees- en schrijfmodel van HBase uitgelegd met de uitleg van Lars Hofhansl\cite{hbase-acid}. De combinatie van een enkele HRegionServer voor een record en het gebruiken van locks, zorgt ervoor dat atomaire acties op een enkele record succesvol afgedwongen kunnen worden.     

Uit de testresultaten blijkt dat indien de leesbewerking te snel verstuurd wordt, er nog geen blokkering van de bewerking zal plaats vinden. Het percentage van de queries dat vanaf de eerste keer al de juist data zal lezen, bevindt zich in tabel \ref{table:consistentie-hbase-correct}.

\begin{table}
\centering
\begin{tabular}{l| l l}
	\textbf{Lezer} & \textbf{Starttijdstip (ms)} & \textbf{Percentage eerste keer} \\
	\hline
	1 &  0 ms & 2.6\%\\
	2 &  3 ms & 68\%\\
	3 &  6 ms & 90\%\\
	4 &  9 ms & 93\%\\
	5 &  12 ms & 94\%\\
	6 &  15 ms & 96\%\\
	7 &  18 ms & 96\%\\
	8 &  21 ms & 96\%\\
	9 &  24 ms & 97\%\\
	10 & 27 ms & 97\% 
\end{tabular}
\caption{Consistentie: Percentage van de queries dat van de eerste keer de juiste data leest voor HBase. Het gemiddelde verschil tussen het starten en stoppen van het lezen op een willekeurig record is ongeveer 6ms. }
\label{table:consistentie-hbase-correct}
\end{table}

\begin{figure}[tb!]
	\begin{minipage}{0.5\textwidth} 
	\textbf{Schrijven}
	\begin{enumerate}
	\item Lock de rij(en), om te beschermen tegen concurrente schrijfacties. 
	\item Haal het huidige schrijfnummer op
	\item Voeg aanpassingen toe aan WAL (Write Ahead Log)
	\item Pas aanpassing toe op de Memstore (cache geheugen)
	\item Commit de transactie, m.a.w. zet het leespunt op het nieuwe schrijfnummer
	\item Unlock de rijen
	\end{enumerate}
	\end{minipage} \hfill
	\begin{minipage}{0.3\textwidth} 
	\textbf{Lezen}
	\begin{enumerate}
	\item Open de lezer
	\item Ga naar het huidige leespunt
	\item Filter al de KeyValues paren met schrijfnummer > leespunt
	\item Sluit de lezer
	\end{enumerate}
	\end{minipage}
	\caption{HBase: Het vereenvoudigde lees- en schrijfmodel voor strikte consistentie in HBase naar Lars Hofhansl\cite{hbase-acid}}\label{fig:consistency-hbase-uitleg}
\end{figure}

\paragraph{MongoDB} MongoDB biedt strikte consistentie aan als er van de primary gelezen wordt maar er zijn ook andere schrijf- en leesmethodes. Een verschil met HBase is dat het bij alle mogelijke lees- en schrijfmethodes mogelijk is om de nieuwe data al te lezen vooraleer de schrijfbewerking beëindigd is. Een schrijf query wacht op de server nog na het schrijven en vrijgeven van zijn schrijf lock. Een verklaring is hiervoor niet gevonden.

Een analyse van de data uit figuur \ref{fig:consistentie-mongodb-all}, kan tonen hoeveel percent kans er is dat data consistent gelezen zal worden vanaf een bepaald tijdstip.  Voor de waarden van 0, 2, 4, 6 en 8 ms is dit berekend, respectievelijk lezer 1 tot 5,  en kunnen de waarden teruggevonden worden in \ref{table:consistentie-mongodb-correct}. Het is opvallend dat met uitzondering van het lezen van de dichtstbijzijnde, er geen groot verschil is tussen de verschillende percentages van verschillende schrijfoperaties. De schrijf configuraties geven dus geen garanties tijdens het uitvoeren maar enkel na de voltooiing van de schrijfoperatie.  Na verder onderzoek blijkt het verschil bij nearest niet significant te zijn doordat er net iets meer lezers een primary op dat moment als dichtstbijzijnde hadden. 

Uit tabel \ref{table:consistentie-mongodb-inconsistency} blijkt dat het in MongoDB niet is gegarandeerd dat als een lezer de nieuwe waarde leest, dat al de overige lezers dat ook zullen doen. In dit geval was het schrijven nog niet voltooid en een bepaalde lezer leest de nieuwe data al, een latere bewerking leest de oude waarde nog. Dit kan verklaard worden doordat het verschillende servers zijn waarop gelezen wordt. Maar aangezien de MongoDB driver periodiek controleert welke server het dichtste bij is, kan dit juist tussen deze 2 bewerkingen gebeuren als men niet de leesgarantie op primary zet. In dit geval is er géén garantie op monotone leesbewerkingen. 

Aangezien er in de testomgeving een uniforme netwerkvertraging is naar alle instanties, volgt de data de veronderstelling dat de dichtstbijzijnde node in iets minder 1/3 van de gevallen een primary is en iets meer dan 2/3 een secondary. Met 5\% afwijking is het moeilijk te stellen dat deze significant is. 

Tenslotte hebben primary en primary-preferred in deze testen geen significante verschillen. Dit komt omdat de primary heel de tijd beschikbaar is. 

\begin{table}
\centering
\begin{tabular}{l | l l l l}
Lezer & Start lezen (ms) & Stop lezen (ms) & Gelezen waarde & Correct? \\
\hline
\multirow{2}{*}{1} & 2,200 & 3,213 & 12553\textbf{3}813315 & Nee\\
 & 13,426 & 14,279 & 12553\textbf{4}813315 & Ja \\
 \multirow{2}{*}{3} & 17,458 & 18,834 & 12553\textbf{3}813315 & Nee\\
 & 29,063 & 29,897 & 12553\textbf{3}813315& Ja \\
\end{tabular}
\caption{Consistentie: Ruwe data van MongoDB test waarbij inconsistente data wordt gelezen na het lezen van consistente data op verschillende lezers met het lezen via nearest en schrijven via fsync\_safe}
\label{table:consistentie-mongodb-inconsistency}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{L{2.3cm}| L{2.3cm} L{2.3cm} L{2.3cm} L{2.3cm}}
& \parbox[t]{2.3cm}{nearest} & \parbox[t]{2.3cm}{primary} & \parbox[t]{2.3cm}{primary- \newline preferred} & \parbox[t]{2.3cm}{secondary }\\ 
  \hline
safe & \parbox[t]{2.3cm}{28, 69, 89, 91, 92} & \parbox[t]{2.3cm}{80, 98, 98, 99, 99} & \parbox[t]{2.3cm}{74, 99, 99, 99, 99} & \parbox[t]{2.3cm}{0, 65, 83, 85, 88 }\\ 
  normal & \parbox[t]{2.3cm}{24, 68, 87, 89, 92} & \parbox[t]{2.3cm}{72, 99, 100, 100, 100} & \parbox[t]{2.3cm}{75, 98, 98, 98, 98} & \parbox[t]{2.3cm}{0, 69, 85, 89, 92 }\\ 
  fsync\_safe & \parbox[t]{2.3cm}{28, 73, 87, 90, 90} & \parbox[t]{2.3cm}{68, 96, 98, 98, 98} & \parbox[t]{2.3cm}{78, 97, 98, 98, 98} & \parbox[t]{2.3cm}{0, 66, 80, 85, 86 }\\ 
  replicas\_safe & \parbox[t]{2.3cm}{24, 74, 87, 88, 91} & \parbox[t]{2.3cm}{75, 98, 99, 99, 99} & \parbox[t]{2.3cm}{79, 98, 98, 98, 98} & \parbox[t]{2.3cm}{1, 67, 84, 87, 89 }\\ 
  majority & \parbox[t]{2.3cm}{26, 77, 91, 91, 92} & \parbox[t]{2.3cm}{73, 98, 99, 99, 99} & \parbox[t]{2.3cm}{77, 99, 99, 99, 100} & \parbox[t]{2.3cm}{0, 61, 82, 85, 89 }\\ 
\end{tabular}
	\caption{Consistentie: Percentage van de queries dat van de eerste keer juist de data leest bij 0ms, 2ms, 4ms, 6ms en 8ms voor MongoDB. Met als rijen de verschillende schrijf types en als kolommen de verschillende lees types. De gemiddelde vertraging op een onafhankelijke leesoperatie is 1ms. }
	\label{table:consistentie-mongodb-correct}
\end{table}

\paragraph{Conclusie} Beide database systemen bieden strikte consistentie aan maar hebben een verschillende uitwerking hiervan: bij HBase worden de leesoperaties uitgesteld tot de volledige voltooiing van de schrijfoperatie, bij MongoDB zal de data al vroeger beschikbaar zijn. Beide systemen zijn \textit{session} consistent en \textit{read-your-own-write} consistent indien er op een primary wordt gelezen voor MongoDB. 

\textit{Session}, \textit{read-your-own-write}, \textit{casual} en \textit{monotonic} consistentie zijn niet gegarandeerd in MongoDB indien er niet gelezen wordt op een primaire. De MongoDB driver kan op ieder moment een andere server kiezen in deze gevallen en kan dus nog oude data lezen. HBase heeft deze garanties wel. 

Het falen van de primary tijdens de schrijfoperaties kan de consistentie garanties beïnvloeden, een nieuwe primary kan verkozen worden die de data nog niet had ontvangen. Maar een gebruiker zou de data al wel van de oude primary gelezen kunnen zijn, in dit geval faalt hier de stikte consistentie. Dit gedrag is wel niet getest en bevestigd. HBase heeft deze situatie niet door de keuze om de leesbewerking te verlengen, een gebruiker dient dus langer te wachten op zijn data. 

\section{Conclusie}
De drie systemen hebben verschillende aanpak naar beschikbaarheid en consistentie. Pgpool-II is het minst geavanceerd systeem door geen automatisch herstel te ondersteunen, maar door de centrale aanpak van de toegangsnode gebruikt dit systeem geen netwerk verkeer als het niet wordt gebruikt. 

MongoDB is een systeem dat weinig configureerbaar is naar het gedrag bij het falen van een instantie, daarentegen zijn er een verschillende configuratiemogelijkheid naar het lees- en schrijfgedrag. Alhoewel het strikte consistentie zegt aan te bieden, kunnen er vraagtekens bij gezet worden. Enkel als er gelezen wordt van de primary, zal er strikte consistentie zijn, waar het nog onduidelijk is welke garanties er zijn bij het falen van een primary. Daarnaast is het in normale situaties mogelijk om de nieuwe data snel te kunnen lezen. 

HBase is met behulp van Zookeeper configureerbaar naar het gedrag bij falen van een enkele instantie, de onbeschikbaarheidsperiode kan verkleind of vergroot worden. De consistentie garanties van HBase zijn strikt voor een enkel record maar dit komt wel voor de prijs dat een leesactie uitgesteld wordt indien er een de schrijfactie op dat record uitgevoerd wordt. 